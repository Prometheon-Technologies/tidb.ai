{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple rag for generating evaluation case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def search_by_semantic(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Query the similar chunks by semantic.\n",
    "\n",
    "    :return: The API response as a List object if successful, otherwise an error message.\n",
    "    List of:\n",
    "     - text_content\n",
    "     - source_uri\n",
    "     - source_name\n",
    "     - relevance_score\n",
    "    \"\"\"\n",
    "    url = \"https://tidb.ai/api/v1/indexes/default/retrieve\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    data = {\n",
    "        \"top_k\": top_k,\n",
    "        \"search_top_k\": 150,\n",
    "        \"query\": query,\n",
    "    }\n",
    "\n",
    "    max_retries = 20  # Maximum number of retries\n",
    "    retry_delay = 5  # Delay between retries in seconds\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "            response.raise_for_status()  # This will raise an HTTPError if the response was an error\n",
    "            return response.json() # Return the successful response as JSON\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            if err.response.status_code == 500 or err.response.status_code == 504:\n",
    "                print(f\"Attempt {attempt + 1} of {max_retries}: HTTP {err.response.status_code} Server Error - {err.response}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                # Properly raising an exception with a formatted message\n",
    "                raise\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} of {max_retries}: Request Error. Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    # If the loop completes without returning, raise an exception\n",
    "    raise Exception(\"Error: Max retries reached. The request failed to complete successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "\n",
    "from llama_index.core.schema import (\n",
    "    NodeWithScore,\n",
    "    TextNode,\n",
    ")\n",
    "\n",
    "@observe()\n",
    "def generation_bench(id: str, question: str, chunks:List[str]) -> str:\n",
    "    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function\n",
    "    langfuse_handler = langfuse_context.get_current_llama_index_handler()\n",
    "    Settings.callback_manager = CallbackManager([langfuse_handler])\n",
    "\n",
    "    nodes = [\n",
    "        NodeWithScore(\n",
    "            node=TextNode(\n",
    "                text=chunk\n",
    "            )\n",
    "        ) for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    response = \"\"\n",
    "    with Settings.callback_manager.as_trace(id):\n",
    "        synthesizer = CompactAndRefine()\n",
    "        response = synthesizer.synthesize(question, nodes)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_case(question):\n",
    "    relevant_chunks = search_by_semantic(question)\n",
    "    relevant_chunks_df = pd.DataFrame(relevant_chunks)\n",
    "    chunks = [chunk for chunk in relevant_chunks_df['text']]\n",
    "    response = generation_bench(\"test\", question, chunks)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TiDB is an advanced open-source, distributed SQL database engineered to support Hybrid Transactional and Analytical Processing (HTAP) workloads.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"what's tidbï¼Ÿ\",\n",
    "]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    answer = run_case(question)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "oai = OpenAI()\n",
    "\n",
    "\n",
    "def call_openai_evaluation(output, expected_output):\n",
    "    instruction = (\n",
    "       \"You are a teacher, one of your job is to evalute student's answer based on the reference answer which is correct.\\n\"\n",
    "       \"your evaluation result should be a score between 0-10, 10 is completely correct, and 0 is complete wrong. You don't need to pursue the same words, as long as the meaning is the same.\"\n",
    "    )\n",
    "\n",
    "    user_message = (\n",
    "        f\"student answer:\\n{output}\\n\\n\"\n",
    "        f\"reference answer (correct answer):\\n{expected_output}\\n\\n\"\n",
    "        \"Now, please begin to evaluate whether student answer is correct, output your evaluation into a json List {'score': ...}!\"\n",
    "    )\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": instruction,\n",
    "    },{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message,\n",
    "    }]\n",
    "\n",
    "    response = oai.chat.completions.create(\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=messages,\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def simple_evaluation(output, expected_output):\n",
    "  response =  call_openai_evaluation(output, expected_output)\n",
    "  data = json.loads(response)\n",
    "  return data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langfuse import Langfuse\n",
    " \n",
    "# init\n",
    "langfuse = Langfuse()\n",
    " \n",
    "def run_app(input):\n",
    "  generationStartTime = datetime.now()\n",
    " \n",
    "  openai_completion = generation_bench(input['args'][0], input['args'][1], input['args'][2])\n",
    " \n",
    "  langfuse_generation = langfuse.generation(\n",
    "    name=\"tidb.ai llm generation\",\n",
    "    input=input,\n",
    "    output=openai_completion,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    start_time=generationStartTime,\n",
    "    end_time=datetime.now()\n",
    "  )\n",
    " \n",
    "  return openai_completion, langfuse_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(experiment_name):\n",
    "  dataset = langfuse.get_dataset(\"answer generation\")\n",
    " \n",
    "  for item in dataset.items:\n",
    "    completion, langfuse_generation = run_app(item.input)\n",
    " \n",
    "    item.link(langfuse_generation, experiment_name) # pass the observation/generation object or the id\n",
    " \n",
    "    langfuse_generation.score(\n",
    "      name=\"relevance\",\n",
    "      value=simple_evaluation(completion, item.expected_output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langfuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
